{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "directory_path = '/home/mind/projects/projects/action_recognition/data'\n",
    "\n",
    "df = pd.DataFrame(columns=['Nose_x', 'Nose_y', 'Left-eye_x', 'Left-eye_y', 'Right-eye_x', 'Right-eye_y', 'Left-ear_x', 'Left-ear_y','Right-ear_x', 'Right-ear_y', 'Left-shoulder_x', 'Left-shoulder_y', 'Right-shoulder_x', 'Right-shoulder_y', 'Left-elbow_x', 'Left-elbow_y', 'Right-elbow_x', 'Right-elbow_y', 'Left-wrist_x', 'Left-wrist_y', 'Right-wrist_x', 'Right-wrist_y', 'Left-hip_x', 'Left-hip_y', 'Right-hip_x', 'Right-hip_y', 'Left-knee_x', 'Left-knee_y', 'Right-knee_x', 'Right-knee_y', 'Left-ankle_x', 'Left-ankle_y', 'Right-ankle_x', 'Right-ankle_y', 'Class'])\n",
    "\n",
    "for sub_directory in os.listdir(directory_path):\n",
    "\n",
    "    sub_directory_path = os.path.join(directory_path, sub_directory)\n",
    "    if sub_directory.lower() == 'standing':\n",
    "        # class_encoding = np.array([1,0,0])\n",
    "        class_encoding = 0\n",
    "    elif sub_directory.lower() == 'falling':\n",
    "        # class_encoding = np.array([0,1,0])\n",
    "        class_encoding = 1\n",
    "    elif sub_directory.lower() == 'walking':\n",
    "        # class_encoding = np.array([0,0,1])\n",
    "        class_encoding = 2\n",
    "\n",
    "    for image in os.listdir(sub_directory_path):\n",
    "        image_path = os.path.join(sub_directory_path, image)\n",
    "        # img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.imread(image_path)\n",
    "        # plt.imshow(img)\n",
    "        # plt.show()\n",
    "        results = model(source=img,\n",
    "                show=False,\n",
    "                conf=0.3,\n",
    "                save=False,\n",
    "                verbose=False,\n",
    "                stream=True,\n",
    "        )\n",
    "        for result in results:\n",
    "            keypoints = result.keypoints.xyn[0].numpy()\n",
    "            df.loc[len(df.index)] = [keypoints[0][0],\n",
    "                                     keypoints[0][1],\n",
    "                                     keypoints[1][0],\n",
    "                                     keypoints[1][1],\n",
    "                                     keypoints[2][0],\n",
    "                                     keypoints[2][1],\n",
    "                                     keypoints[3][0],\n",
    "                                     keypoints[3][1],\n",
    "                                     keypoints[4][0],\n",
    "                                     keypoints[4][1],\n",
    "                                     keypoints[5][0],\n",
    "                                     keypoints[5][1],\n",
    "                                     keypoints[6][0],\n",
    "                                     keypoints[6][1],\n",
    "                                     keypoints[7][0],\n",
    "                                     keypoints[7][1],\n",
    "                                     keypoints[8][0],\n",
    "                                     keypoints[8][1],\n",
    "                                     keypoints[9][0],\n",
    "                                     keypoints[9][1],\n",
    "                                     keypoints[10][0],\n",
    "                                     keypoints[10][1],\n",
    "                                     keypoints[11][0],\n",
    "                                     keypoints[11][1],\n",
    "                                     keypoints[12][0],\n",
    "                                     keypoints[12][1],\n",
    "                                     keypoints[13][0],\n",
    "                                     keypoints[13][1],\n",
    "                                     keypoints[14][0],\n",
    "                                     keypoints[14][1],\n",
    "                                     keypoints[15][0],\n",
    "                                     keypoints[15][1],\n",
    "                                     keypoints[16][0],\n",
    "                                     keypoints[16][1],\n",
    "                                     class_encoding]  \n",
    "   \n",
    "    print('----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.drop(columns='Class')\n",
    "y_train = df['Class']\n",
    "# print(x_train, y_train)\n",
    "\n",
    "# x_train = df.iloc[:, :17].values\n",
    "# y_train = df.iloc[:, -1].values\n",
    "# print(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf', C=0.1, gamma=0.1, degree=3)\n",
    "clf = svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single person image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(source='test_data/standing0.jpg',\n",
    "                show=False,\n",
    "                conf=0.3,\n",
    "                save=False,\n",
    "                verbose=False,\n",
    "                stream=True,\n",
    "        )\n",
    "\n",
    "for output in outputs:\n",
    "    output_keypoints = output.keypoints.xyn.numpy()\n",
    "    # print(output_keypoints)\n",
    "\n",
    "prediction = clf.predict(output_keypoints.flatten().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single person video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/mind/projects/projects/action_recognition/test_data/production_id_5136708(720p).mp4')\n",
    "\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to read video. Please check path again')\n",
    "\n",
    "while(cap.isOpened):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "\n",
    "        outputs = model(source=frame,\n",
    "                show=True,\n",
    "                conf=0.6,\n",
    "                save=False,\n",
    "                verbose=False,\n",
    "                stream=True,\n",
    "        )\n",
    "\n",
    "        for output in outputs:\n",
    "            output_keypoints = output.keypoints.xyn.numpy()\n",
    "        \n",
    "        text = ''\n",
    "        flag = False\n",
    "\n",
    "        if len(output_keypoints[0]) == 17:\n",
    "            prediction = clf.predict(output_keypoints[0].flatten().reshape(1, -1))\n",
    "            flag = True\n",
    "\n",
    "        if flag:\n",
    "            if prediction[0] == float(0):\n",
    "                text = 'standing'\n",
    "            elif prediction[0] == float(1):\n",
    "                text = 'falling'\n",
    "            elif prediction[0] == float(2):\n",
    "                text = 'walking'\n",
    "\n",
    "        frame = cv2.putText(frame.copy(), text, (425, 100), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=[0, 0, 255], thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('output', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple people video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/mind/projects/projects/action_recognition/test_data/pexels-gabby-k-6220073(720p).mp4')\n",
    "\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to read video. Please check path again')\n",
    "\n",
    "frame_width = 720\n",
    "frame_height = 1080\n",
    "    \n",
    "# frame_width = int(cap.get(3))\n",
    "# frame_height = int(cap.get(4))\n",
    "# frame_size = (frame_width,frame_height)\n",
    "\n",
    "video_writer = cv2.VideoWriter(\"/home/mind/projects/projects/action_recognition/test_data/outputs/output_pexels-gabby-k-6220073(720p).mp4\",\n",
    "                               cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                               int(cap.get(5)),\n",
    "                               (frame_width, frame_height))\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "\n",
    "        frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "        \n",
    "        frame_count += 1\n",
    "        # print(f'Frame {frame_count}')\n",
    "\n",
    "        outputs = model(source=frame,\n",
    "                show=False,\n",
    "                conf=0.5,\n",
    "                save=False,\n",
    "                verbose=False,\n",
    "                stream=False,\n",
    "        )\n",
    "\n",
    "        for output in outputs:\n",
    "            output_keypoints = output.keypoints.xyn.numpy()\n",
    "            output_boxes = output.boxes.xywh.numpy()\n",
    "            # print(f'keypoints: {output_keypoints}')\n",
    "            # print(f'boxes: {output_boxes}')\n",
    "        \n",
    "        for i in range(len(output_boxes)):\n",
    "            text = ''\n",
    "            flag = False\n",
    "\n",
    "            #if len(output_keypoints[0]) == 17:\n",
    "            prediction = clf.predict(output_keypoints[i].flatten().reshape(1, -1))\n",
    "            flag = True\n",
    "\n",
    "            if flag:\n",
    "                if prediction[0] == float(0):\n",
    "                    text = 'standing'\n",
    "                elif prediction[0] == float(1):\n",
    "                    text = 'falling'\n",
    "                elif prediction[0] == float(2):\n",
    "                    text = 'walking'\n",
    "\n",
    "            # print((output_boxes[i][0], output_boxes[i][1]))\n",
    "            frame = cv2.putText(frame.copy(), text, (int(output_boxes[i][0]), int(output_boxes[i][1])), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[0, 0, 255], thickness=2, lineType=cv2.LINE_AA)\n",
    "            # frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "            cv2.imshow('output', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "                break\n",
    "        \n",
    "        video_writer.write(frame) \n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "video_writer.release() \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "action_recognition_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
